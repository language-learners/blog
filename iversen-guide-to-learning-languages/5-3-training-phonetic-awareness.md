---
layout: page
title: 5.3 Training phonetic awareness
author: Iversen
---
All languages have a phonetic level and a phonematic level. The phonetic level is the level with the actual sounds emitted by people who speak, while the phonematic level is a simplified version of this, where the concrete sounds are assigned to a small number of building blocks called phonemes. Two sounds from a given language belong to different phonemes if you can find at least one word or word combination with different meaning in that language (a so-called minimal pair) which only differ by having one or the other of these sounds.  If you can't find such a pair then even fairly different sounds can be assigned to the same phoneme. Such sounds are then called allophones.

This means that if you transcribe a spoken passage using phonemes you can't be absolute sure how it should sound. There may be some rules of thumb which tell which allophones to use in a given context, but at the end of the day you have to learn the actual pronunciation by listening to native speakers. If you know something about the sound system of a given language you will have an easier time identifying the phonemes, which is necessary because it is the combination of  the phonemes that produce meaningful elements like words. The phonetic level may contribute to the meaning at a more global level, but the phonemes are defined in such a way that they suffice to describe all the meaning bearing units of a language.

If you read a standard book about the phonology of a language you will be presented with a lot of details about the way the phonemes are represented as allophones in different environments, and you will learn about the relevant mouth positions and probably also something about dialectal variations. But we don't all study academic books about phonology. They are notoriously dry and hard reading - especially if you don't already have the 'tone' of a certain language ringing in your ears. So instead we just rely on the lists of sounds in textbooks or language guides to help us along, and that is not quite enough. 

The main problem is that we have to do a phonematic reduction of the information to get the meaning of the things we here, and in this process we deliberately disregard the details and variations and intonations and everything else that isn't absolutely necessary for the choice between different meanings. And this situation is made worse by the way such lists use sounds of the base language of the book to represent the sounds of the target language - even in cases where the similarity isn't too overwhelming. And because the whole thing has to be written down the idiosyncrasies of the base language make the lists even harder to deal with.

Not to single any particular language out, but English is a particularly problematic base language because it has such a pathetic writing system and so many local variations. For instance you have to write "oo" to indicate a flat /u/ sound, and this makes a transcription of for instance Bahasa Indonesia harder to read than the original (this language has an almost perfect phonetic writing system). For instance a guidebook ("buku petunjuk") is rendered as "*booˑkoo peˑtoonˑjook*" in Lonely Planet Indonesian from 2006. 

So phonetic awareness means going back to the original sounds and listening to them. You need to know which words were intended in order to know which phonemes are intended, but at this level the global meaning of a sentence itself isn't important. If we speak about intonation you do however need to consider larger blocks of speech and you need to know the grammatical structure, but even here the general meaning constitutes a background information rather than your study object. 

The most efficient technique I have used for studying minute differences - below the phonematic level - is problematic because it demands concentration and absence of outside disturbances. The idea is that you find a short snippet of speech which you can repeat again and again while you try to write down exactly what you hear. Ideally you should do this in a recognized transcription system like IPA, but then you have to learn that first. I use homebrewed 'alphabets' and keep telling myself that this is OK because I don't have to communicate the results to others - the important thing being the listening process itself. 

Of course I have listened to speech before and done some parroting of the usual kind, but seeing how long time it takes me really to hear what is said is shocking because it tells me how much I have missed earlier - even when I have replayed a certain passage several times. It is like walking from platform 9 to 10 at King's Cross, and suddenly you discover that there is something weird in between. Let's take a few concrete examples: 

*Is príomhoide teagaisc mé*  
*/Ish pri·vɔdjə tægæshχmæ/*  
*Is! principal (of)teaching I*  
*I am a teaching principal*

The first line is an original Irish phrase, which I put into the excellent Irish speech synthesizer [Abair](http://abair.ie/), using the Cabóigín voice. Below in the second line is my impression of what the result sounded like, written in my own totally idiosyncratic sound writing. Oh yes, I ought to learn the IPA system which is the norm for transcriptions in the scientific world, but it would take me a lot of time to get as comfortable with that as I am with my own homegrown systems - and we are not studying Irish here, so the details aren't important here. The third line is a hyperliteral translation as described in part 2, and the fourth line contains a 'normal' translation into standard English. 

Another example: I took a sentence from the homepage of Burgers Zoo in Arnhem, the Netherlands, and then I let four different voices from the [Acapela Box](http://www.acapela-group.com/text-to-speech-interactive-demo.html) read it aloud. The result, which I published on HTLAL in 2012, clearly shows how much variation there is between different speakers - a lesson which language learners and teachers definitely should take seriously:

U kunt tegenwoordig een combi-kaart voor Burgers' Zoo en het Nederlands Openluchtmuseum kopen 

*/e könt teχəvo·Rdeχ e·n kɔmbikαRRt fo·R BöRhɔrs so· æn ət ne·dɔlansə-öpənleχtmyseåm kåubə/*  
*/y könt teχəvo·RRdeχ e·n kɔmbikαRRt fo·R BöRgɔs so· æn-ət ne·dɔla·Rns opɔlöχtmyseöm ko·pə/*  
*/y könt teχəvo·Rdeχ en kɔmbikαRt fo·R BøRgɔs so· æn (h)ət ne·dɔlansə opɔ(R)löRχtmyzæöm kåbə/*  
*/y könt teχəvɔ·Rdeχ e·n kɔmbikαRRt fo·R BøRχɔRs so· æn ət ne·dɔla(R)ns opɔ(R)löRχtmyzæjöm kåupə/*

It takes surprisingly long time to catch the subtleties in even a few sentences, but after you have done it you are better equipped to hear what really is going on in genuine speech. And then you  can translate the rough indications in dictionaries etc. to the actual sounds which you have heard with your own ears.  And yes, I ought to use some internationally recognized standard system for the transcription task, but I feel more at home with my own homebrewed systems - which to boot differ from language to language.

But few people do this exercise, and instead they rely on an automatic osmosis mechanism which means that the correct pronunciations seep into their system, where they replace erroneous preconceptions based upon standard orthography and even the supposedly phonetical spellings in dictionaries etc. And this doesn't always work out as intended because you hear what you expect to hear and not what really is said. You may be one of the exceptional individuals who can remember and repeat any spoken sequence with 100% precision, but don't count on it. 

Some readers may at this point ask themselves how I can trust a speech synthesizer. Well I can't, but much depend on the quality of the synthesizer, and it is practical to have a machine that accepts everything you tell it to say, and who can do it again and again using exactly the same pronunciation

OK we have discussed single sounds, but at the other end of the scale you find the suprasegmental intonation patterns, which almost never are indicated in language learning materials - you are (once again) expected to absorb them from the things you listen to through some kind of unconscious osmosis. Again there is one thing you can do to make this process partly conscious: get some text in both a spoken and a written version and print out the written version with large spaces between the lines. Now mark up two things: stress patterns and tone level (in practice you may have to do one thing at a time). 

Write a wavering coloured line for the tone level - and check this against the type of sentence. Is it a question? An order? A triumphant conclusion? Sometimes there a jumps in the tone level which seemingly don't have any grammatical explanation, but they may still be necessary for the authentic 'nativelike' sound. 

This is a useful exercise with any language, but especially important for languages heavy use of sentence 'melody' - like Swedish and Norwegian. And one of the funny experiences is of course when something unexpected happens in the pronunciation. Just as an experiment I took a question from a Swedish Youtube video and put it into Acapela, and then I drew the following lines:

![Intonation comparison of one human Swedish pronunciation, four artificial Swedish voices, and three Norwegian ones.](../5-3-image.jpg)

Only the first of these lines shows the actual intonation as spoken by a living human being, but the seven others definitely sound like Swedes resp. Norwegians in spite of being compiled from sampled fragments by a machine. And at least one of the Norwegian voices actually shows the archetypical jump upwards at the end of a sentence - which in this case also is a question. However  the software must have overlooked this when it produced the flat intonation pattern of the first Norwegian voice, or the speaker speaks a dialect with an intonation pattern that is closer to Swedish than it is to other Norwegian dialects. 

These lines only indicate tone level, but the distribution of stress points can of course also be notated in a number of ways (including the use of accent signs in the writing system itself). With the stress patterns the main error is to assume that there are a few localized stressed syllables and then no stress at all around them. But there are many levels of stress, and different languages have different patterns - some have large variations, some have small variations and some have regular intervals like telegraph poles in a landscape, others aren't nearly as regular. We all try to learn these patterns by listening and listening and listening, but my claim is that you can listen more efficiently when you have made clear what you are listening for. And having to notate exactly what you hear is an excellent way of forcing yourself to listen carefully, which carries over into your listening to authentic speech later on. Again I have to say that you may be one of the lucky ones who can absorb sounds without ever studying them in detail, but we aren't all born with this talent.



Next section: [5.4 To know NOTHING but still understand something](../5-4-to-know-nothing-but-still-understand-something/)  
Go to [content index](../)
